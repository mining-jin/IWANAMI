##第12章　特徴量選択
##pp.274-275
##パッケージFSelectionを用いた変数の情報利得と利得比の計算の例
path<-"http://mjin.doshisha.ac.jp/data/NatsuIzumi1.csv"
NatsuIzumi1<-read.csv(path, row.names=1)#mac OSの場合：NatsuIzumi1<-read.csv(path, row.names=1, fileEncoding="shift-JIS")
install.packages("FSelector")
library(FSelector)
weight<-information.gain(authors~., NatsuIzumi1[, 1:10])
weight
subset<-cutoff.k(weight, 3)
subset
##関数cutoff.biggest.diffを用いて大きい順に変数項目が抽出できる．
subset<-cutoff.biggest.diff(weight)
subset
##抽出したものを分類器に適応させるには次のようにすると便利である．
f<-as.simple.formula(subset, "authors")
print(f)
##選択した変数を用いた線形判別分析の例
library(MASS)
lda(f, NatsuIzumi1)
##選択した変数を用いた主成分分析の例
biplot(prcomp(NatsuIzumi1[, subset]))

##pp.276
##AUCによる特徴量選択の例
library(caret)
ni.lab<-as.factor(NatsuIzumi1[, 1])
fvi<-filterVarImp(NatsuIzumi1[, 2:10], ni.lab)
head(fvi)
fvi #値が大きいほど重要度が増す．
                 #表12.4の(a)のAUCとは一致しない．
fvis2<-fvi[sort.list(fvi[, 1], dec=TRUE), ] #大きい順にソートする
fvis2

##pp.279-281
##Borutaを用いた変数選択の例
path<-"http://mjin.doshisha.ac.jp/data/NatsuIzumi2.csv"
NatsuIzumi2<-read.csv(path, row.names=1)#mac OSの場合：NatsuIzumi2<-read.csv(path, row.names=1, fileEncoding="shift-JIS")
install.packages("Boruta")
library(Boruta)
NI<-as.matrix(NatsuIzumi2[, -1]) 　 #マトリックス型に変換
NI.lab<-as.factor(NatsuIzumi2[, 1]) #ラベルを因子型に変換
Bor<-Boruta(NI[, 2:14], NI.lab, ntree=10000)
par(mar=c(8, 4, 4, 4))
plot(Bor, las=2, xlab="")           #図12.1の作成
grid()
print(Bor)
exatts<-attStats(Bor)               #計算結果をデータフレームに返す
##6番目の列に各変数について決定（Confirmed），暫定（Tentative），棄却（Rejected）の情報が付与されている．
##この情報を用いて棄却された変数を除いたデータセットを次のように作成することができる．
colLabCon<-rownames(exatts[exatts[, 6]=="Confirmed", ])
colLabTen<-rownames(exatts[exatts[, 6]=="Tentative", ])
NI2<-NI[, c(colLabCon, colLabTen)]  #選択された変数のデータセット

##すべての変数を用いた結果を次に示す
dim(NatsuIzumi2)
Bor<-Boruta(NI, NI.lab, ntree=10000)
print(Bor)

##pp.280-281
##varImp{caret}を用いたRRFによる特徴量選択の例
library(caret)
set.seed(1)
rrf.Mod<-train(authors~., data=NatsuIzumi2, method="RRF")
rrf.Imp<-varImp(rrf.Mod, scale=TRUE)
rrf.Imp
plot(rrf.Imp, top=20, main="Variable Importance")