#上記の結果を計算
path<-'https://github.com/mining-jin/IWANAMI/blob/master/data/Novel200_1087.csv'
akuta<-read.csv(path, row.names=1, fileEncoding="Shift-JIS") #データの読み込み
akuta.lm0<-lm(y~., akuta)  #モデルの作成
akuta.lm1<-step(akuta.lm0) #変数の選択
summary(akuta.lm1).        #回帰結果の要約の表示

#回帰モデルの回帰係数の棒グラフの作成
coef<-sort(coefficients(akuta.lm1)[-1])
par(mar=c(4, 10, 2, 2))
dotchart(coef, gpch=21, bg=2, col=1, pt.cex=1.5, lcolor=1)

#pp.259
library(glmnet)
X<-as.matrix(akuta[,-1]) #説明変数をマトリックスに変換
Y<-as.matrix(akuta[,1])  #目的変数をマトリックスに変換
aku.cv<-cv.glmnet(X, Y, alpha=1) #交差検証をする
aku.cv$lambda.min.               #残差二乗の和が最小値のλ
#[1] 0.1062251.                  #これが残差二乗の和が最小となるλ
log(aku.cv$lambda.min)           #横軸の座標値
plot(aku.cv)                     #図12.4の作成
##予測は，重回帰分析と同じく関数predictを用いる．
predict(aku.cv, X).              #学習に用いたデータXを用いている！

#ゼロより大きい係数を出力する．
coef.min<-coef(aku.cv, s="lambda.min") #誤差最小のモデルの係数
temp[temp[,1]>0,] #ゼロより大きい係数を返す．

#αとλについてグリッドサーチ
set.seed(12345)
cv10 = trainControl(method = "cv", number = 10)
tuneGrid <-expand.grid(alpha = seq(0, 1, 0.1), lambda = seq(0, 1, 0.1))
elnet <- train(
  y~., data = akuta,
  method = "glmnet",
  trControl = cv10,
  tuneGrid = tuneGrid
)
elnet
## RMSE（Root Mean Squared Error）最小となるのはα=0.1，λ=0.4である．
## この値で求められたモデルは elnet に保存されている．
## 予測は次のように行う．
elnet[[6]] #このように，選択されたαとλのみを返すこともできる．
predict(elnet, X)

##モデルの回帰係数を返す．
akuta.gl<-glmnet(X, Y, alpha = 0.1, lambda = 0.4)
predict(akuta.gl, type = "coef")

#ランダムフォレストの例
library(randomForest)
akuta.rf<-randomForest(y~., akuta)
varImpPlot(akuta.rf)
##主な三つの変数の考察
par(mfrow = c(1, 3))
lpa<-list(col = "red", lty = 2, lwd = 2)
scatter.smooth(1:113, akuta$は．副助詞．, lpars = lpa)
scatter.smooth(1:113, akuta$が．格助詞．, lpars = lpa)
scatter.smooth(1:113, akuta$で．格助詞．, lpars = lpa)

#図12.8を作成するスクリプト
library("philentropy") #jensen-shannon距離を用いるため
rekidai<-read.delim("clipboard", row.names=1)
rekidaip<-rekidai/apply(rekidai, 1, sum)
reki.jsd<-distance(rekidaip, method = "jensen-shannon")
rownames(reki.jsd)<-rownames(rekidaip)
reki.hca<-hclust(as.dist(reki.jsd), "ward.D2")
plot(reki.hca)
rect.hclust(reki.hca, k=3, border="red")

#図12.9を作成するスクリプト
library(ldatuning)
library(topicmodels)
tun<-FindTopicsNumber(rekidai, topic = 2:6, method = "Gibbs",
                      metrics = c("Griffiths2004", "CaoJuan2009", 
                                  "Arun2010", "Deveaud2014"))
FindTopicsNumber_plot(tun)

#トピック数の推定の例
rekidaiK<-searchK(documents = rekidai_dfm$documents,
                  vocab = rekidai_dfm$vocab, K = 2:6,
                  data = rekidai_dfm$meta)
plot(rekidaiK) #推定結果の図示

#経年情報を用いたトピックモデルのスクリプトと結果
##トピック分析（線形推定）
rekidai_stm <- stm(documents = rekidai_dfm$documents,
                   vocab = rekidai_dfm$vocab, K = 3,
                   prevalence = ~Year, ###この式が合っているか？ 
                   data = rekidai_dfm$meta, 
                   verbose = FALSE)
summary(rekidai_stm)
##トピック毎の特徴語は4つの指標で示されている．Highest Probは，それぞれのトピック内で現れる確率が高い語であり，FREXはトピック別の特徴を示す語である．図12.10で示しているのはFREX指標に基づいているものである．
##トピック毎の回帰モデルの推定（年代が説明変数である）
topic_estimate <- estimateEffect(formula = 1:3~Year,
                                 stmobj = rekidai_stm,
                                 metadata = rekidai_dfm$meta)
summary(topic_estimate)
plot(topic_estimate, "Year", method = "continuous", topics = 1:3, model = rekidai_stm)

##トピック分析（非線形推定）
rekidai_stm2 <- stm(documents = rekidai_dfm$documents,
                    vocab = rekidai_dfm$vocab, K = 3,
                    prevalence = ~s(Year, 3),
                    data = rekidai_dfm$meta,
                    verbose = FALSE)
##図12.11の作成．
labeltype = c("prob", "frex", "lift", "score")
plot(rekidai_stm, type = "labels", labeltype = "frex")
##トピック毎の回帰モデルの推定
topic_estimate2 <- estimateEffect(formula = 1:3 ~ s(Year, 3),
                                  stmobj = rekidai_stm2,
                                  metadata = rekidai_dfm$meta)
summary(topic_estimate2)
##図12.11の作成
plot(topic_estimate2, "Year", method = "continuous", topics = 1:3)
##二つのトピック間の特徴後比較は次の関数で図示できる．
plot(rekidai_stm2, type = "perspectives", topics = c(1, 2))

