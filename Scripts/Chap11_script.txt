##第11章　テキストデータを用いた予測
##pp.242
##上記の結果の計算
path<-"http://mjin.doshisha.ac.jp/data/JYO113.csv"
akuta<-read.csv(path, row.names=1) #データの読み込み；#mac OSの場合：akuta<-read.csv(path, row.names=1, fileEncoding="shift-JIS")
akuta.lm0<-lm(y~., akuta)          #モデルの作成
akuta.lm1<-step(akuta.lm0)         #変数の選択
summary(akuta.lm1)                 #回帰結果の要約の表示

##pp.243
##重要な変数のドットプロット作成
coef<-sort(coefficients(akuta.lm1)[-1])
par(mar=c(4, 10, 2, 2))
dotchart(coef, gpch=21, bg=2, col=1, pt.cex=1.5, lcolor=1)
#ドットプロット作成
install.packages("lattice")
library(lattice)
dotplot(coef, las=2, horiz=T, cex.names=0.9)

##pp.248
##交差確認法によるλの推定
install.packages("glmet")
library(glmnet)
X<-as.matrix(akuta[, -1]) #説明変数をマトリックスに変換
Y<-as.matrix(akuta[, 1])  #目的変数をマトリックスに変換
aku.cv<-cv.glmnet(X, Y, alpha=1) #交差検証をする
aku.cv$lambda.min         #残差二乗の和が最小値のλ
#[1] 0.1062251            #これが残差二乗の和が最小となるλ
log(aku.cv$lambda.min)    #横軸の座標値
plot(aku.cv)              #図11.4の作成
##予測は関数predictを用いる
predict(aku.cv, X)        #学習に用いたデータXを用いている

##pp.249
##ゼロより大きい係数を出力する．
coef.min<-coef(aku.cv, s="lambda.min") #誤差最小のモデルの係数
coef.min[coef.min[, 1]>0, ]            #ゼロより大きい係数を返す

##pp.249-250
##αとλについてグリッドサーチ
install.packages("caret")
library(caret)
set.seed(12345)
cv10=trainControl(method="cv", number=10)
tuneGrid<-expand.grid(alpha=seq(0, 1, 0.1), lambda=seq(0, 1, 0.1))
elnet<-train(
  y~., data=akuta,
  method="glmnet",
  trControl=cv10,
  tuneGrid=tuneGrid
)
elnet
##RMSE(Root Mean Squared Error)最小となるのはα=0.1, λ=0.4となる．
##標本サイズが小さいと実行ごとに結果が揺れる．
##この値で求められたモデルはelnetに保存されている．
##予測は次のように行う．
elnet[[6]] #このように選択されたαとλのみを返すこともできる．
predict(elnet) #学習データを用いた予測値を返す
predict(elnet, X[3:5, ])
#モデルに寄与度が高い重要な項目は以下のように返すことができる．
varImp(elnet)
plot(varImp(elnet), 15, cex=1.5) #上位15個のドットプロットを作成

##モデルの回帰係数を返す．
akuta.gl<-glmnet(X, Y, alpha=0.1, lambda=0.4)
predict(akuta.gl, type="coef")

##pp.253
##ランダムフォレストの例
library(randomForest)
akuta.rf<-randomForest(y~., akuta)
varImpPlot(akuta.rf)
##主な三つの変数の考察
par(mfrow=c(1, 3))
lpa<-list(col="red", lty=2, lwd=2)
scatter.smooth(1:113, akuta$は.副助詞., lpars=lpa)
scatter.smooth(1:113, akuta$が.格助詞., lpars=lpa)
scatter.smooth(1:113, akuta$で.格助詞., lpars=lpa)

##pp.255
##図11.9を作成するスクリプト
install.packages("philentropy")
library(philentropy) #jensen-shannon距離を求めるときに用いる
rekidai.path<-"https://mjin.doshisha.ac.jp/data/rekidai.csv"
rekidai<-read.csv(rekidai.path, row.names=1)#mac OSの場合：rekidai<-read.csv(rekidai.path, row.names=1, fileEncoding="shift-JIS")
rekidaip<-rekidai/apply(rekidai, 1, sum)
reki.jsd<-distance(rekidaip, method="jensen-shannon")
rownames(reki.jsd)<-rownames(rekidaip) #結果にラベルを付ける
reki.hca<-hclust(as.dist(reki.jsd), "ward.D2")
plot(reki.hca)
rect.hclust(reki.hca, k=3, border="red")

##pp.256
##図11.10を作成するスクリプト
library(ldatuning)
library(topicmodels)
tun<-FindTopicsNumber(rekidai, topics=2:6,
                      method="Gibbs",
                      metrics=c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
)
FindTopicsNumber_plot(tun)

##pp.257-258
##トピックの推定の例
install.packages(c("tidyverse", "magrittr", "stringr", "tm", "stm"))
library(tm)
library(tidyverse)
library(magrittr)
library(stringr)
library(stm)
#データフレーム型のデータを関数stmに適した構造に変換する．
rekidai2<-as.matrix(rekidai[, -ncol(rekidai)])
rekidai3<-as.DocumentTermMatrix(rekidai2, weighting=weightTf)
rekidai_dfm<-readCorpus(rekidai3, type="slam")
##読み込んだテキストの行のラベル前の4文字が所信表明演説の西暦の年度であるため，頭の4文字を切り取り目的変数ベクトルを作成する．
year<-names(rekidai_dfm$documents)%>%str_sub(1, 4)
rekidai_dfm$meta<-list(Year=as.numeric(year))#目的変数を格納する

#関数searchKを用いてトピック数の推定を行う．
rekidaiK<-searchK(documents=rekidai_dfm$documents,
                  vocab=rekidai_dfm$vocab, K=2:8,
                  data=rekidai_dfm$meta)
plot(rekidaiK)  #推定結果のグラフの作成

##pp.260-262
##経年情報を用いたトピックモデルのスクリプトと結果
##トピック分析（線形推定）
rekidai_stm<-stm(documents=rekidai_dfm$documents,
                 vocab=rekidai_dfm$vocab, K=3,
                 prevalence=~Year,
                 data=rekidai_dfm$meta, verbose=FALSE)
summary(rekidai_stm)
##トピック毎の回帰モデルの推定（年度が説明変数である）
topic_estimate<-estimateEffect(formula=1:3~Year,
                               stmobj=rekidai_stm,
                               metadata=rekidai_dfm$meta)
summary(topic_estimate)
plot(topic_estimate, "Year", method="continuous", topics=1:3, model=rekidai_stm)

##トピック分析（非線形推定）
rekidai_stm2<-stm(documents=rekidai_dfm$documents,
                  vocab=rekidai_dfm$vocab, K=3,
                  prevalence=~s(Year, 3),
                  data=rekidai_dfm$meta, verbose=FALSE)

##図11.12の作成．labeltype=c("prob", "frex", "lift", "score")
plot(rekidai_stm, type="labels", labeltype="frex")

##トピック毎の回帰モデルの推定
plot(rekidai_stm, type="labels", labeltype="frex")

##トピック毎の回帰モデルの推定
topic_estimate2<-estimateEffect(formula=1:3~s(Year, 3),
                                stmobj=rekidai_stm2,
                                metadata=rekidai_dfm$meta)
summary(topic_estimate2)
##図11.13の作成
plot(topic_estimate2, "Year", method="continuous", topics=1:3)
##二つのトピック間の特徴語比較は次の関数で図示できる．
plot(rekidai_stm2, type="perspectives", topics=c(1, 2))